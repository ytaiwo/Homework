{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is a neural network? What are the general steps required to build a neural network?\n",
    "\n",
    "A neural network is an artifical network based on a brain network. A neural network has many layers where the first layer is the input layer and the last layer is the output layer. Each layer, from the first layer, feeds into the next layer. The general steps to building a neural network is to identify input and target values from your dataset. Identify the number of hidden layers, train your neural network, activation of the neural network using a model that goes best with what you're trying to predict. Finally, testing the neural network to see if it's a good fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tGenerally, how do you check the performance of a neural network? Why? \n",
    "\n",
    "Check the performance of a neural network using RMSE, MSE, Mean absolute error, accuracy, and precision and recall. These all tell your how well the neural network predicts the target variable. Based on the performance metric, the neural network can be tuned to make it perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3       4       5       6      7   8\n",
       "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
       "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
       "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
       "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
       "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "abalone_df = pd.read_table(\"../week_17/abalone.data\", sep=\",\",header=None)\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_wt</th>\n",
       "      <th>Shucked_wt</th>\n",
       "      <th>Viscera_wt</th>\n",
       "      <th>Shell_wt</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole_wt  Shucked_wt  Viscera_wt  Shell_wt  \\\n",
       "0   M   0.455     0.365   0.095    0.5140      0.2245      0.1010     0.150   \n",
       "1   M   0.350     0.265   0.090    0.2255      0.0995      0.0485     0.070   \n",
       "2   F   0.530     0.420   0.135    0.6770      0.2565      0.1415     0.210   \n",
       "3   M   0.440     0.365   0.125    0.5160      0.2155      0.1140     0.155   \n",
       "4   I   0.330     0.255   0.080    0.2050      0.0895      0.0395     0.055   \n",
       "\n",
       "   Rings   Age  \n",
       "0     15  16.5  \n",
       "1      7   8.5  \n",
       "2      9  10.5  \n",
       "3     10  11.5  \n",
       "4      7   8.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df.columns=[\"Sex\",\"Length\",\"Diameter\",\"Height\",\"Whole_wt\",\"Shucked_wt\",\"Viscera_wt\",\"Shell_wt\",\"Rings\"]\n",
    "#to predict age add 1.5\n",
    "abalone_df[\"Age\"] = abalone_df[\"Rings\"] + 1.5\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_wt</th>\n",
       "      <th>Shucked_wt</th>\n",
       "      <th>Viscera_wt</th>\n",
       "      <th>Shell_wt</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole_wt  Shucked_wt  Viscera_wt  Shell_wt  \\\n",
       "0   M   0.455     0.365   0.095    0.5140      0.2245      0.1010     0.150   \n",
       "1   M   0.350     0.265   0.090    0.2255      0.0995      0.0485     0.070   \n",
       "2   F   0.530     0.420   0.135    0.6770      0.2565      0.1415     0.210   \n",
       "3   M   0.440     0.365   0.125    0.5160      0.2155      0.1140     0.155   \n",
       "4   I   0.330     0.255   0.080    0.2050      0.0895      0.0395     0.055   \n",
       "\n",
       "   Rings   Age  F  I  M  \n",
       "0     15  16.5  0  0  1  \n",
       "1      7   8.5  0  0  1  \n",
       "2      9  10.5  1  0  0  \n",
       "3     10  11.5  0  0  1  \n",
       "4      7   8.5  0  1  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode Sex column using one hot encoding\n",
    "dum_df = pd.get_dummies(abalone_df['Sex'])\n",
    "abalone_df = abalone_df.join(dum_df)\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Sex column after one hot encoding\n",
    "abalone_df = abalone_df.drop('Sex',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_wt</th>\n",
       "      <th>Shucked_wt</th>\n",
       "      <th>Viscera_wt</th>\n",
       "      <th>Shell_wt</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>9.933684</td>\n",
       "      <td>11.433684</td>\n",
       "      <td>0.312904</td>\n",
       "      <td>0.321283</td>\n",
       "      <td>0.365813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>3.224169</td>\n",
       "      <td>3.224169</td>\n",
       "      <td>0.463731</td>\n",
       "      <td>0.467025</td>\n",
       "      <td>0.481715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>2.825500</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Length     Diameter       Height     Whole_wt   Shucked_wt  \\\n",
       "count  4177.000000  4177.000000  4177.000000  4177.000000  4177.000000   \n",
       "mean      0.523992     0.407881     0.139516     0.828742     0.359367   \n",
       "std       0.120093     0.099240     0.041827     0.490389     0.221963   \n",
       "min       0.075000     0.055000     0.000000     0.002000     0.001000   \n",
       "25%       0.450000     0.350000     0.115000     0.441500     0.186000   \n",
       "50%       0.545000     0.425000     0.140000     0.799500     0.336000   \n",
       "75%       0.615000     0.480000     0.165000     1.153000     0.502000   \n",
       "max       0.815000     0.650000     1.130000     2.825500     1.488000   \n",
       "\n",
       "        Viscera_wt     Shell_wt        Rings          Age            F  \\\n",
       "count  4177.000000  4177.000000  4177.000000  4177.000000  4177.000000   \n",
       "mean      0.180594     0.238831     9.933684    11.433684     0.312904   \n",
       "std       0.109614     0.139203     3.224169     3.224169     0.463731   \n",
       "min       0.000500     0.001500     1.000000     2.500000     0.000000   \n",
       "25%       0.093500     0.130000     8.000000     9.500000     0.000000   \n",
       "50%       0.171000     0.234000     9.000000    10.500000     0.000000   \n",
       "75%       0.253000     0.329000    11.000000    12.500000     1.000000   \n",
       "max       0.760000     1.005000    29.000000    30.500000     1.000000   \n",
       "\n",
       "                 I            M  \n",
       "count  4177.000000  4177.000000  \n",
       "mean      0.321283     0.365813  \n",
       "std       0.467025     0.481715  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.000000     0.000000  \n",
       "75%       1.000000     1.000000  \n",
       "max       1.000000     1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3781, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove outliers\n",
    "Q1 = abalone_df.quantile(0.25)\n",
    "Q3 = abalone_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "abalone_df = abalone_df[~((abalone_df < (Q1 - 1.5 * IQR)) |(abalone_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "abalone_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense\n",
    "\n",
    "X = abalone_df.drop(['Age','Rings'],axis=1)\n",
    "y = abalone_df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae','RootMeanSquaredError'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "265/265 [==============================] - 1s 1ms/step - loss: 111.2902 - mae: 10.2758 - root_mean_squared_error: 10.5379\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 30.6196 - mae: 5.0843 - root_mean_squared_error: 5.4950\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 5.0649 - mae: 1.7129 - root_mean_squared_error: 2.2499\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 4.5475 - mae: 1.6290 - root_mean_squared_error: 2.1316\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 4.0598 - mae: 1.5247 - root_mean_squared_error: 2.0132\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.7427 - mae: 1.4879 - root_mean_squared_error: 1.9338\n",
      "Epoch 7/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.6500 - mae: 1.4663 - root_mean_squared_error: 1.9097\n",
      "Epoch 8/100\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 3.6047 - mae: 1.4526 - root_mean_squared_error: 1.8977\n",
      "Epoch 9/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.4621 - mae: 1.4261 - root_mean_squared_error: 1.8600\n",
      "Epoch 10/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 3.2729 - mae: 1.3847 - root_mean_squared_error: 1.8087\n",
      "Epoch 11/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 3.2701 - mae: 1.3802 - root_mean_squared_error: 1.8074\n",
      "Epoch 12/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.1407 - mae: 1.3595 - root_mean_squared_error: 1.7697\n",
      "Epoch 13/100\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 3.2243 - mae: 1.3865 - root_mean_squared_error: 1.7952\n",
      "Epoch 14/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.2213 - mae: 1.3831 - root_mean_squared_error: 1.7940\n",
      "Epoch 15/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.0557 - mae: 1.3451 - root_mean_squared_error: 1.7472\n",
      "Epoch 16/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.9981 - mae: 1.3350 - root_mean_squared_error: 1.7314\n",
      "Epoch 17/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.0158 - mae: 1.3229 - root_mean_squared_error: 1.7358\n",
      "Epoch 18/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 3.1440 - mae: 1.3733 - root_mean_squared_error: 1.7720\n",
      "Epoch 19/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 3.0017 - mae: 1.3371 - root_mean_squared_error: 1.7317\n",
      "Epoch 20/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.9115 - mae: 1.3170 - root_mean_squared_error: 1.7061\n",
      "Epoch 21/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.9947 - mae: 1.3463 - root_mean_squared_error: 1.7294\n",
      "Epoch 22/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.9366 - mae: 1.3292 - root_mean_squared_error: 1.7129\n",
      "Epoch 23/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.8201 - mae: 1.3040 - root_mean_squared_error: 1.6790\n",
      "Epoch 24/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.8362 - mae: 1.3097 - root_mean_squared_error: 1.6838\n",
      "Epoch 25/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.8407 - mae: 1.3045 - root_mean_squared_error: 1.6840\n",
      "Epoch 26/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7465 - mae: 1.2738 - root_mean_squared_error: 1.6555\n",
      "Epoch 27/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7977 - mae: 1.2956 - root_mean_squared_error: 1.6720\n",
      "Epoch 28/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7539 - mae: 1.2889 - root_mean_squared_error: 1.6592\n",
      "Epoch 29/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7056 - mae: 1.2721 - root_mean_squared_error: 1.6435\n",
      "Epoch 30/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7059 - mae: 1.2794 - root_mean_squared_error: 1.6444\n",
      "Epoch 31/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7177 - mae: 1.2652 - root_mean_squared_error: 1.6482\n",
      "Epoch 32/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.5923 - mae: 1.2527 - root_mean_squared_error: 1.6090\n",
      "Epoch 33/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6870 - mae: 1.2651 - root_mean_squared_error: 1.6386\n",
      "Epoch 34/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.8040 - mae: 1.2925 - root_mean_squared_error: 1.6740\n",
      "Epoch 35/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.6810 - mae: 1.2689 - root_mean_squared_error: 1.6371\n",
      "Epoch 36/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6783 - mae: 1.2666 - root_mean_squared_error: 1.6351\n",
      "Epoch 37/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6181 - mae: 1.2634 - root_mean_squared_error: 1.6178\n",
      "Epoch 38/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6618 - mae: 1.2588 - root_mean_squared_error: 1.6311\n",
      "Epoch 39/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6448 - mae: 1.2643 - root_mean_squared_error: 1.6253\n",
      "Epoch 40/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7089 - mae: 1.2666 - root_mean_squared_error: 1.6452\n",
      "Epoch 41/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7032 - mae: 1.2726 - root_mean_squared_error: 1.6434\n",
      "Epoch 42/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6179 - mae: 1.2598 - root_mean_squared_error: 1.6160\n",
      "Epoch 43/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7149 - mae: 1.2885 - root_mean_squared_error: 1.6468\n",
      "Epoch 44/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6148 - mae: 1.2336 - root_mean_squared_error: 1.6159\n",
      "Epoch 45/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5222 - mae: 1.2462 - root_mean_squared_error: 1.5875\n",
      "Epoch 46/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5540 - mae: 1.2511 - root_mean_squared_error: 1.5972\n",
      "Epoch 47/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5692 - mae: 1.2342 - root_mean_squared_error: 1.6024\n",
      "Epoch 48/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6397 - mae: 1.2595 - root_mean_squared_error: 1.6242\n",
      "Epoch 49/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5029 - mae: 1.2280 - root_mean_squared_error: 1.5811\n",
      "Epoch 50/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5488 - mae: 1.2474 - root_mean_squared_error: 1.5961\n",
      "Epoch 51/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6439 - mae: 1.2588 - root_mean_squared_error: 1.6236\n",
      "Epoch 52/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6068 - mae: 1.2410 - root_mean_squared_error: 1.6142\n",
      "Epoch 53/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5165 - mae: 1.2294 - root_mean_squared_error: 1.5862\n",
      "Epoch 54/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.6281 - mae: 1.2561 - root_mean_squared_error: 1.6206\n",
      "Epoch 55/100\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 2.5670 - mae: 1.2375 - root_mean_squared_error: 1.6017\n",
      "Epoch 56/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.5587 - mae: 1.2356 - root_mean_squared_error: 1.5984\n",
      "Epoch 57/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5667 - mae: 1.2357 - root_mean_squared_error: 1.6018\n",
      "Epoch 58/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.3944 - mae: 1.2045 - root_mean_squared_error: 1.5468\n",
      "Epoch 59/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4100 - mae: 1.2054 - root_mean_squared_error: 1.5509\n",
      "Epoch 60/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.6395 - mae: 1.2556 - root_mean_squared_error: 1.6238\n",
      "Epoch 61/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4722 - mae: 1.2211 - root_mean_squared_error: 1.5719\n",
      "Epoch 62/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5243 - mae: 1.2327 - root_mean_squared_error: 1.5881\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4436 - mae: 1.2054 - root_mean_squared_error: 1.5620\n",
      "Epoch 64/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5581 - mae: 1.2307 - root_mean_squared_error: 1.5988\n",
      "Epoch 65/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.7065 - mae: 1.2858 - root_mean_squared_error: 1.6434\n",
      "Epoch 66/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.6010 - mae: 1.2509 - root_mean_squared_error: 1.6111\n",
      "Epoch 67/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.4771 - mae: 1.2209 - root_mean_squared_error: 1.5726\n",
      "Epoch 68/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.6171 - mae: 1.2313 - root_mean_squared_error: 1.6168\n",
      "Epoch 69/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.5287 - mae: 1.2433 - root_mean_squared_error: 1.5897\n",
      "Epoch 70/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.3296 - mae: 1.1935 - root_mean_squared_error: 1.5255A: 0s - loss: 2.2053 - mae: 1.1734 - root_mean_squared_error:  - ETA: 0s - loss: 2.3229 - mae: 1.1923 - root_mean_squared_error: 1.523\n",
      "Epoch 71/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.3985 - mae: 1.2049 - root_mean_squared_error: 1.5471A: 0s - loss: 2.3183 - mae: 1.1838 - root_mean_squared_error: \n",
      "Epoch 72/100\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 2.4477 - mae: 1.2179 - root_mean_squared_error: 1.5640\n",
      "Epoch 73/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3991 - mae: 1.2089 - root_mean_squared_error: 1.5487\n",
      "Epoch 74/100\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.4366 - mae: 1.2149 - root_mean_squared_error: 1.5607\n",
      "Epoch 75/100\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 2.4759 - mae: 1.2066 - root_mean_squared_error: 1.5733A: 0s - loss: 2.4927 - mae: 1.2048 - root_mean_squared_error: 1\n",
      "Epoch 76/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4188 - mae: 1.2012 - root_mean_squared_error: 1.5544\n",
      "Epoch 77/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3710 - mae: 1.1923 - root_mean_squared_error: 1.5396\n",
      "Epoch 78/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3870 - mae: 1.1959 - root_mean_squared_error: 1.5446\n",
      "Epoch 79/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4308 - mae: 1.1946 - root_mean_squared_error: 1.5586\n",
      "Epoch 80/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5691 - mae: 1.2467 - root_mean_squared_error: 1.6023\n",
      "Epoch 81/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3651 - mae: 1.1884 - root_mean_squared_error: 1.5364\n",
      "Epoch 82/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4031 - mae: 1.2084 - root_mean_squared_error: 1.5492\n",
      "Epoch 83/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4487 - mae: 1.2184 - root_mean_squared_error: 1.5641\n",
      "Epoch 84/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5567 - mae: 1.2305 - root_mean_squared_error: 1.5976\n",
      "Epoch 85/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4753 - mae: 1.2207 - root_mean_squared_error: 1.5729\n",
      "Epoch 86/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5086 - mae: 1.2251 - root_mean_squared_error: 1.5829\n",
      "Epoch 87/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3011 - mae: 1.1644 - root_mean_squared_error: 1.5140\n",
      "Epoch 88/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5451 - mae: 1.2478 - root_mean_squared_error: 1.5948\n",
      "Epoch 89/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4229 - mae: 1.2046 - root_mean_squared_error: 1.5561\n",
      "Epoch 90/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3975 - mae: 1.2057 - root_mean_squared_error: 1.5476\n",
      "Epoch 91/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3769 - mae: 1.1978 - root_mean_squared_error: 1.5412\n",
      "Epoch 92/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3580 - mae: 1.1998 - root_mean_squared_error: 1.5352\n",
      "Epoch 93/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4256 - mae: 1.2117 - root_mean_squared_error: 1.5568\n",
      "Epoch 94/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3649 - mae: 1.1894 - root_mean_squared_error: 1.5372\n",
      "Epoch 95/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5183 - mae: 1.2361 - root_mean_squared_error: 1.5857\n",
      "Epoch 96/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4739 - mae: 1.2133 - root_mean_squared_error: 1.5725\n",
      "Epoch 97/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.5134 - mae: 1.2353 - root_mean_squared_error: 1.5849\n",
      "Epoch 98/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.3572 - mae: 1.1875 - root_mean_squared_error: 1.5342\n",
      "Epoch 99/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4744 - mae: 1.2236 - root_mean_squared_error: 1.5727\n",
      "Epoch 100/100\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 2.4788 - mae: 1.2170 - root_mean_squared_error: 1.5739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8591ab190>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWrite another algorithm to predict the same result as the previous question using either KNN or logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  3.200528634361233\n",
      "Root Mean Squred Error:  1.7890021336938737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#KNN model - using regressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "y_predict = knn.predict(X_test)\n",
    "\n",
    "knn_mse = MSE(y_test,y_predict)\n",
    "knn_rmse = knn_mse ** (1/2)\n",
    "\n",
    "print(\"Mean Squared Error: \", knn_mse)\n",
    "print(\"Root Mean Squred Error: \", knn_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tCreate a neural network using pytorch to predict the same result as question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "\n",
    "X = abalone_df.drop(['Age','Rings'],axis=1).values\n",
    "y = abalone_df['Age'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "y_train = y_train.float()\n",
    "y_train = y_train.view(-1,1)\n",
    "\n",
    "y_test = y_test.float()\n",
    "y_test = y_test.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=10, hidden1=20, hidden2=20, out_features =2):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 109.35504150390625\n",
      "Epoch number: 11 with loss: 87.32950592041016\n",
      "Epoch number: 21 with loss: 28.671648025512695\n",
      "Epoch number: 31 with loss: 13.375268936157227\n",
      "Epoch number: 41 with loss: 4.560524940490723\n",
      "Epoch number: 51 with loss: 5.384434700012207\n",
      "Epoch number: 61 with loss: 4.519216537475586\n",
      "Epoch number: 71 with loss: 4.03368616104126\n",
      "Epoch number: 81 with loss: 3.947730302810669\n",
      "Epoch number: 91 with loss: 3.8341777324676514\n",
      "Epoch number: 101 with loss: 3.720399856567383\n",
      "Epoch number: 111 with loss: 3.6262707710266113\n",
      "Epoch number: 121 with loss: 3.545247793197632\n",
      "Epoch number: 131 with loss: 3.472867727279663\n",
      "Epoch number: 141 with loss: 3.408388376235962\n",
      "Epoch number: 151 with loss: 3.3517091274261475\n",
      "Epoch number: 161 with loss: 3.3024284839630127\n",
      "Epoch number: 171 with loss: 3.260009288787842\n",
      "Epoch number: 181 with loss: 3.2238121032714844\n",
      "Epoch number: 191 with loss: 3.193145990371704\n",
      "Epoch number: 201 with loss: 3.1672816276550293\n",
      "Epoch number: 211 with loss: 3.145484447479248\n",
      "Epoch number: 221 with loss: 3.1270368099212646\n",
      "Epoch number: 231 with loss: 3.111269474029541\n",
      "Epoch number: 241 with loss: 3.097581624984741\n",
      "Epoch number: 251 with loss: 3.085448741912842\n",
      "Epoch number: 261 with loss: 3.074437379837036\n",
      "Epoch number: 271 with loss: 3.064199924468994\n",
      "Epoch number: 281 with loss: 3.0544662475585938\n",
      "Epoch number: 291 with loss: 3.0450351238250732\n",
      "Epoch number: 301 with loss: 3.0357606410980225\n",
      "Epoch number: 311 with loss: 3.026541233062744\n",
      "Epoch number: 321 with loss: 3.0173091888427734\n",
      "Epoch number: 331 with loss: 3.008017063140869\n",
      "Epoch number: 341 with loss: 2.9986348152160645\n",
      "Epoch number: 351 with loss: 2.989143133163452\n",
      "Epoch number: 361 with loss: 2.979529619216919\n",
      "Epoch number: 371 with loss: 2.969787120819092\n",
      "Epoch number: 381 with loss: 2.9599123001098633\n",
      "Epoch number: 391 with loss: 2.949901819229126\n",
      "Epoch number: 401 with loss: 2.939753293991089\n",
      "Epoch number: 411 with loss: 2.9294636249542236\n",
      "Epoch number: 421 with loss: 2.919032335281372\n",
      "Epoch number: 431 with loss: 2.9084582328796387\n",
      "Epoch number: 441 with loss: 2.8977439403533936\n",
      "Epoch number: 451 with loss: 2.8868908882141113\n",
      "Epoch number: 461 with loss: 2.8758978843688965\n",
      "Epoch number: 471 with loss: 2.864762783050537\n",
      "Epoch number: 481 with loss: 2.8534929752349854\n",
      "Epoch number: 491 with loss: 2.842094898223877\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras model performed better. With Keras, I obtained a low RMSE of 1.51. With KNN, my lowest RMSE was 1.79, and with pytorch, it was 1.68. Keras probably performaed better because of the ability to add multiple hidden layers to make the model perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
